#!/usr/bin/env bash
# ----------------------------------------------------------------------------
# ref_genomes runner (ad-hoc friendly, simple and transparent)
#
# Steps:
#  1) Read genomes.yaml in the current directory (YAML; JSON also accepted)
#  2) For each genome: download FASTA/annotation (if URLs), decompress FASTA
#  3) If ERCC enabled: concatenate ERCC92 to create <id>_with_ERCC.fa
#  4) If indices requested: run nf-core/references to build them (via --input datasheet)
#  5) Write everything under OUTDIR (default: current directory)
#
# Requirements: bash, python3, PyYAML (for YAML), nextflow, and a container/conda runtime
# ----------------------------------------------------------------------------
set -euo pipefail

# Enable bash command tracing if VERBOSE=1
if [[ "${VERBOSE:-0}" == "1" ]]; then set -x; fi

# ---- CLI arguments ----------------------------------------------------------
# Usage:
#   ./run.sh --all                         # iterate all genomes
#   ./run.sh --only GRCh38,GRCm39          # only selected genomes (comma-separated)
#
# If neither flag is provided, defaults to --all for convenience.

ALL=0
ONLY_LIST=""

usage() {
  cat <<USAGE
ref_genomes runner

Options:
  --all                  Process all genomes listed in genomes.yaml
  --only ID[,ID2,...]    Process only the specified genome IDs (comma-separated)
  -h, --help             Show this help

Examples:
  ./run.sh --all
  ./run.sh --only GRCh38,GRCm39
USAGE
}

while [[ $# -gt 0 ]]; do
  case "$1" in
    --all)
      ALL=1; shift;;
    --only)
      ONLY_LIST="${2:-}"; shift 2;;
    --only=*)
      ONLY_LIST="${1#--only=}"; shift;;
    -h|--help)
      usage; exit 0;;
    *)
      # Unknown argument; stop parsing and continue (allows future extensions)
      break;;
  esac
done

# Default to --all when neither flag is provided
if [[ $ALL -eq 0 && -z "$ONLY_LIST" ]]; then ALL=1; fi

_in_only() {
  local gid="$1"
  # If ONLY_LIST is empty, everything is allowed (covers --all)
  [[ -z "$ONLY_LIST" ]] && return 0
  IFS=',' read -r -a _arr <<< "$ONLY_LIST"
  for x in "${_arr[@]}"; do
    [[ "$gid" == "$x" ]] && return 0
  done
  return 1
}

# ---- Configuration knobs (override via environment) -------------------------
OUTDIR="${OUTDIR:-$PWD}"             # Where outputs go (use your --out path)
CFG_PATH="${CFG_PATH:-$PWD/genomes.yaml}"  # Config file
THREADS="${THREADS:-16}"
MEM_GB="${MEM_GB:-64}"
NXF_PROFILE="${NXF_PROFILE:-docker}" # docker | singularity | conda
NF_PIPELINE="${NF_PIPELINE:-nf-core/references}"
NF_REVISION="${NF_REVISION:-dev}"
NF_EXTRA_ARGS="${NF_EXTRA_ARGS:-}"

# Default set of indices if none specified for a genome
ALL_INDICES="star,bwa,bwamem2,bowtie2,hisat2,minimap2,salmon,kallisto"

# ERCC bundle lives next to this script
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
ERCC_FA="$SCRIPT_DIR/ERCC92/ERCC92.fa"

# ---- Small helpers ----------------------------------------------------------
have() { command -v "$1" >/dev/null 2>&1; }
log()  { printf "[%s] %s\n" "$(date -u +%FT%TZ)" "$*"; }

download_if_url() {
  # $1=url-or-path  $2=dest
  local src="$1" dest="$2"
  if [[ "$src" =~ ^https?:// ]]; then
    log "download: $src -> $dest"
    mkdir -p "$(dirname "$dest")"
    if have curl; then curl -L --fail --retry 3 -o "$dest" "$src"; else wget -O "$dest" "$src"; fi
  else
    # Local path: copy to dest for reproducibility
    mkdir -p "$(dirname "$dest")"; cp -f "$src" "$dest"
  fi
}

decompress_to() {
  # $1=maybe.gz  $2=plain.fa
  local in="$1" out="$2"
  mkdir -p "$(dirname "$out")"
  if [[ "$in" =~ \.gz$ ]]; then
    log "decompress: $in -> $out"; gzip -c -d "$in" > "$out"
  else
    cp -f "$in" "$out"
  fi
}

run_nfcore_references() {
  # $1=datasheet.yml  $2=aligners  $3=tx_aligners  $4=outdir
  local datasheet="$1" aligners="$2" txaln="$3" outdir="$4"
  mkdir -p "$outdir/work"
  log "nextflow: input=$(basename "$datasheet") (aligners=$aligners tx=$txaln)"
  NXF_DEFAULT_HOME="$outdir/.nxf" \
  nextflow run "$NF_PIPELINE" -r "$NF_REVISION" -profile "$NXF_PROFILE" \
    --input "$datasheet" \
    ${aligners:+--aligners "$aligners"} \
    ${txaln:+--transcriptome_aligners "$txaln"} \
    --outdir "$outdir" \
    --max_cpus "$THREADS" \
    --max_memory "${MEM_GB}GB" \
    -work-dir "$outdir/work" \
    $NF_EXTRA_ARGS
}

# ---- Read config (YAML preferred, JSON allowed) -----------------------------
# We parse with Python for clarity and print simple TSV lines for bash to read.
mapfile -t GENOMES < <(python3 - "$CFG_PATH" << 'PY'
import json, sys, os
path = sys.argv[1]
cfg = None
if path.endswith('.json'):
    with open(path) as fh: cfg = json.load(fh)
else:
    try:
        import yaml  # type: ignore
        with open(path) as fh: cfg = yaml.safe_load(fh)
    except Exception as e:
        print(f"YAML parse failed: {e}. Install PyYAML or use JSON.", file=sys.stderr)
        sys.exit(2)
if not isinstance(cfg, dict) or 'genomes' not in cfg:
    print('Invalid config: missing top-level "genomes" list', file=sys.stderr)
    sys.exit(3)
with_ercc = cfg.get('with_ercc', True)
nf = cfg.get('nfcore', {})
out_root = cfg.get('out_root')  # may be None
print(f"GLOBAL\t{with_ercc}\t{nf.get('pipeline','')}\t{nf.get('revision','')}\t{nf.get('profile','')}\t{nf.get('extra_args','')}\t{out_root or ''}")
for g in (cfg.get('genomes') or []):
    gid = g.get('id') or ''
    fasta = g.get('fasta') or ''
    gtf = g.get('gtf') or ''
    indices = ','.join(g.get('indices') or [])
    ercc = g.get('ercc')
    ercc_s = '' if ercc is None else ('true' if ercc else 'false')
    print(f"GENOME\t{gid}\t{fasta}\t{gtf}\t{indices}\t{ercc_s}")
PY
)

# First line is GLOBAL settings
IFS=$'\t' read -r _ WITH_ERCC CFG_PIPELINE CFG_REV CFG_PROFILE CFG_EXTRA CFG_OUT_ROOT <<< "${GENOMES[0]}"

# Effective settings (env overrides config; config overrides script defaults)
OUT_ROOT="${CFG_OUT_ROOT:-$OUTDIR}"
NF_PIPELINE="${NF_PIPELINE:-${CFG_PIPELINE:-$NF_PIPELINE}}"
NF_REVISION="${NF_REVISION:-${CFG_REV:-$NF_REVISION}}"
NXF_PROFILE="${NXF_PROFILE:-${CFG_PROFILE:-$NXF_PROFILE}}"
NF_EXTRA_ARGS="${NF_EXTRA_ARGS:-${CFG_EXTRA:-$NF_EXTRA_ARGS}}"

log "Output root: $OUT_ROOT"
mkdir -p "$OUT_ROOT"

# Remaining lines are genomes
for i in "${!GENOMES[@]}"; do
  [ "$i" -eq 0 ] && continue
  IFS=$'\t' read -r _ GID FASTA GTF INDICES ERCC_OVERRIDE <<< "${GENOMES[$i]}"
  [ -n "$GID" ] || { log "skip genome with missing id"; continue; }
  [ -n "$FASTA" ] || { log "skip $GID: missing fasta"; continue; }

  # Selection filter: respect --only when provided (takes precedence over --all)
  if ! _in_only "$GID"; then
    log "skip $GID: not selected via --only"
    continue
  fi

  log "=== $GID ==="
  local_root="$OUT_ROOT/$GID"
  mkdir -p "$local_root/src"

  # 1) Stage FASTA and GTF into src/
  fasta_staged="$local_root/src/$(basename "$FASTA")"
  download_if_url "$FASTA" "$fasta_staged"
  gtf_staged=""; if [ -n "$GTF" ]; then gtf_staged="$local_root/src/$(basename "$GTF")"; download_if_url "$GTF" "$gtf_staged"; fi

  # 2) Decompress FASTA to plain for concatenation/indexing (the pipeline accepts local paths)
  fasta_plain="$local_root/src/$(basename "${fasta_staged%.gz}")"
  decompress_to "$fasta_staged" "$fasta_plain"

  # 3) ERCC augmentation
  use_ercc="$WITH_ERCC"; [ -n "$ERCC_OVERRIDE" ] && use_ercc="$ERCC_OVERRIDE"
  ercc_plain=""; if [[ "$use_ercc" =~ ^(true|True)$ ]]; then
    ercc_plain="$local_root/src/${GID}_with_ERCC.fa"
    log "ERCC: ${GID} + ERCC92 -> $(basename "$ercc_plain")"
    cat "$fasta_plain" "$ERCC_FA" > "$ercc_plain"
  fi

  # 4) Build indices (if requested; default to all tools when empty)
  tools="$INDICES"; [ -z "$tools" ] && tools="$ALL_INDICES"
  if [ -n "$tools" ]; then
    # Split transcriptome tools for nf-core flags
    aligners=$(echo "$tools" | tr ',' '\n' | grep -Ev '^\s*$' | grep -Evi 'salmon|kallisto' | paste -sd, -)
    txaln=$(echo "$tools" | tr ',' '\n' | grep -E 'salmon|kallisto' | paste -sd, -)

    # Prepare a minimal datasheet for the pipeline. It will consume local staged files.
    species_val="${GID}"
    # If user provided species in config, prefer it (optional extension in genomes.yaml)
    # We cannot parse it here reliably without extending the parser; default to GID.

    # Base genome datasheet
    ds_base="$local_root/datasheet.yml"
    cat > "$ds_base" <<YAML
- genome: "$GID"
  species: "$species_val"
  fasta: "$fasta_plain"
$( [ -n "$gtf_staged" ] && printf "  gtf: \"%s\"\n" "$gtf_staged" )YAML
    run_nfcore_references "$ds_base" "$aligners" "$txaln" "$local_root/indices"

    # ERCC indices if we created an augmented FASTA: add a second entry and run into a separate outdir
    if [ -n "$ercc_plain" ]; then
      ds_ercc="$OUT_ROOT/${GID}_with_ERCC/datasheet.yml"
      mkdir -p "$(dirname "$ds_ercc")"
      cat > "$ds_ercc" <<YAML
- genome: "${GID}_with_ERCC"
  species: "$species_val"
  fasta: "$ercc_plain"
$( [ -n "$gtf_staged" ] && printf "  gtf: \"%s\"\n" "$gtf_staged" )YAML
      run_nfcore_references "$ds_ercc" "$aligners" "$txaln" "$OUT_ROOT/${GID}_with_ERCC/indices"
    fi
  else
    log "indices: none requested for $GID"
  fi

  log "done $GID"

done

log "All tasks completed."

# Notes:
# - For first-time Nextflow/nf-core users, validate your setup with a quick test profile:
#     nextflow run nf-core/references -profile test
# - Provide pipeline parameters via CLI or -params-file as recommended by nf-core docs.
# - This runner drives nf-core/references via the recommended --input datasheet.
